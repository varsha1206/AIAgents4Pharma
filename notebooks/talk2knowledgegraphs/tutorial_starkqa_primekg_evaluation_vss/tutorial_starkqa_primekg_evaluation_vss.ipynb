{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarkQA-PrimeKG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will perform a question-and-answering task on the StarkQA-PrimeKG dataset by utilizing textual embeddings over the queries and nodes.\n",
    "\n",
    "The following are important publication and repository links related to this work.\n",
    "- https://arxiv.org/pdf/2404.13207 (latest version)\n",
    "- https://arxiv.org/pdf/2404.13207v2 (prior version)\n",
    "- https://github.com/snap-stanford/stark/blob/main/eval.py\n",
    "- https://github.com/snap-stanford/stark/tree/main/stark_qa/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import ast\n",
    "from typing import Any, Union, List, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics.functional.retrieval.hit_rate import retrieval_hit_rate\n",
    "from torchmetrics.functional.retrieval.reciprocal_rank import retrieval_reciprocal_rank\n",
    "from torchmetrics.functional.retrieval.recall import retrieval_recall\n",
    "from torchmetrics.functional.retrieval.precision import retrieval_precision\n",
    "from torchmetrics.functional.retrieval.average_precision import retrieval_average_precision\n",
    "from torchmetrics.functional.retrieval.ndcg import retrieval_normalized_dcg\n",
    "from torchmetrics.functional.retrieval.r_precision import retrieval_r_precision\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "from aiagents4pharma.talk2knowledgegraphs.datasets.starkqa_primekg import StarkQAPrimeKG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load StarkQA-PrimeKG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StarkQAPrimeKG` allows to load the data from the HuggingFace Hub if the data is not available locally. \n",
    "\n",
    "Otherwise, the data is loaded from the local directory as defined in the `local_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define starkqa primekg data by providing a local directory where the data is stored\n",
    "starkqa_data = StarkQAPrimeKG(local_dir=\"../../../../data/starkqa_primekg/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the dataframes of StarkQA and its split, we just need a method as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StarkQAPrimeKG dataset...\n",
      "../../../../data/starkqa_primekg/qa/prime/stark_qa/stark_qa.csv already exists. Loading the data from the local directory.\n",
      "Loading StarkQAPrimeKG embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awmulyadi/Repositories/office/AIAgents4Pharma/docs/notebooks/talk2knowledgegraphs/../../../aiagents4pharma/talk2knowledgegraphs/datasets/starkqa_primekg.py:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  query_emb_dict = torch.load(query_emb_path)\n",
      "/home/awmulyadi/Repositories/office/AIAgents4Pharma/docs/notebooks/talk2knowledgegraphs/../../../aiagents4pharma/talk2knowledgegraphs/datasets/starkqa_primekg.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  node_emb_dict = torch.load(node_emb_path)\n"
     ]
    }
   ],
   "source": [
    "# Invoke a method to load the data\n",
    "starkqa_data.load_data()\n",
    "\n",
    "# Get the StarkQAPrimeKG data, which are the QA pairs, split indices, and the node information\n",
    "starkqa_split_indices = starkqa_data.get_starkqa_split_indicies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Textual Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `tutorial_starkqa_primekg_textual_embeddings.ipynb` notebook, we have shown how to obtain textual embeddings over the query, node and edge information.\n",
    "\n",
    "Therefore, we just need to load the pre-processed embeddings.\n",
    "\n",
    "We can retrieve query embedding as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>answer_ids</th>\n",
       "      <th>query_embedded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Could you identify any skin diseases associate...</td>\n",
       "      <td>[95886]</td>\n",
       "      <td>[0.050286733, 0.0050845086, 0.06326583, 0.0360...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What drugs target the CYP3A4 enzyme and are us...</td>\n",
       "      <td>[15450]</td>\n",
       "      <td>[0.009708624, 0.01434415, -0.07435164, -0.0736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the name of the condition characterize...</td>\n",
       "      <td>[98851, 98853]</td>\n",
       "      <td>[-0.058651656, -0.0031773308, 0.015822958, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What drugs are used to treat epithelioid sarco...</td>\n",
       "      <td>[15698]</td>\n",
       "      <td>[-0.035772394, 0.064148985, -0.018727051, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Can you supply a compilation of genes and prot...</td>\n",
       "      <td>[7161, 22045]</td>\n",
       "      <td>[-0.072102964, -0.008873461, -0.007186646, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              query      answer_ids  \\\n",
       "0   0  Could you identify any skin diseases associate...         [95886]   \n",
       "1   1  What drugs target the CYP3A4 enzyme and are us...         [15450]   \n",
       "2   2  What is the name of the condition characterize...  [98851, 98853]   \n",
       "3   3  What drugs are used to treat epithelioid sarco...         [15698]   \n",
       "4   4  Can you supply a compilation of genes and prot...   [7161, 22045]   \n",
       "\n",
       "                                      query_embedded  \n",
       "0  [0.050286733, 0.0050845086, 0.06326583, 0.0360...  \n",
       "1  [0.009708624, 0.01434415, -0.07435164, -0.0736...  \n",
       "2  [-0.058651656, -0.0031773308, 0.015822958, -0....  \n",
       "3  [-0.035772394, 0.064148985, -0.018727051, -0.0...  \n",
       "4  [-0.072102964, -0.008873461, -0.007186646, 0.0...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load the enriched nodes dataframe from parquet file\n",
    "starkqa_df = pd.read_parquet(os.path.join(starkqa_data.local_dir, 'starkqaprimekg_queries_embedded.parquet'), engine='pyarrow')\n",
    "\n",
    "# Basic conversion of the answer_ids from string to list\n",
    "starkqa_df['answer_ids'] = starkqa_df.apply(lambda x: ast.literal_eval(x['answer_ids']), axis=1)\n",
    "\n",
    "# Check the dataframe of query embeddings\n",
    "starkqa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can retrieve node embedding as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>node_name</th>\n",
       "      <th>node_type</th>\n",
       "      <th>enriched_node</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PHYHIP</td>\n",
       "      <td>gene/protein</td>\n",
       "      <td>PHYHIP belongs to gene/protein category. Enabl...</td>\n",
       "      <td>[-0.06876933, 0.00096770556, -0.0630331, -0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GPANK1</td>\n",
       "      <td>gene/protein</td>\n",
       "      <td>GPANK1 belongs to gene/protein category. This ...</td>\n",
       "      <td>[-0.08932163, 0.031602174, -0.102335155, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ZRSR2</td>\n",
       "      <td>gene/protein</td>\n",
       "      <td>ZRSR2 belongs to gene/protein category. This g...</td>\n",
       "      <td>[-0.10059608, -0.020288778, 0.008750704, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NRF1</td>\n",
       "      <td>gene/protein</td>\n",
       "      <td>NRF1 belongs to gene/protein category. This ge...</td>\n",
       "      <td>[-0.09837414, -0.02768978, -0.061966445, 0.026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PI4KA</td>\n",
       "      <td>gene/protein</td>\n",
       "      <td>PI4KA belongs to gene/protein category. This g...</td>\n",
       "      <td>[-0.03965294, -0.0017360917, -0.12756099, -5.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   node_id node_name     node_type  \\\n",
       "0        0    PHYHIP  gene/protein   \n",
       "1        1    GPANK1  gene/protein   \n",
       "2        2     ZRSR2  gene/protein   \n",
       "3        3      NRF1  gene/protein   \n",
       "4        4     PI4KA  gene/protein   \n",
       "\n",
       "                                       enriched_node  \\\n",
       "0  PHYHIP belongs to gene/protein category. Enabl...   \n",
       "1  GPANK1 belongs to gene/protein category. This ...   \n",
       "2  ZRSR2 belongs to gene/protein category. This g...   \n",
       "3  NRF1 belongs to gene/protein category. This ge...   \n",
       "4  PI4KA belongs to gene/protein category. This g...   \n",
       "\n",
       "                                                   x  \n",
       "0  [-0.06876933, 0.00096770556, -0.0630331, -0.04...  \n",
       "1  [-0.08932163, 0.031602174, -0.102335155, -0.03...  \n",
       "2  [-0.10059608, -0.020288778, 0.008750704, 0.003...  \n",
       "3  [-0.09837414, -0.02768978, -0.061966445, 0.026...  \n",
       "4  [-0.03965294, -0.0017360917, -0.12756099, -5.2...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load the enriched nodes dataframe from parquet file\n",
    "primekg_nodes = pd.read_parquet(os.path.join(starkqa_data.local_dir, 'starkqaprimekg_nodes_embedded.parquet'), engine='pyarrow')\n",
    "\n",
    "# Check the dataframe of node embeddings\n",
    "primekg_nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure the performance of the model, we need to prepare metrics for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>query_id</th>\n",
       "      <th>pred_rank</th>\n",
       "      <th>mrr</th>\n",
       "      <th>map</th>\n",
       "      <th>rprecision</th>\n",
       "      <th>recall@5</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>recall@100</th>\n",
       "      <th>hit@1</th>\n",
       "      <th>hit@3</th>\n",
       "      <th>hit@5</th>\n",
       "      <th>hit@10</th>\n",
       "      <th>hit@20</th>\n",
       "      <th>hit@50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [idx, query_id, pred_rank, mrr, map, rprecision, recall@5, recall@10, recall@20, recall@50, recall@100, hit@1, hit@3, hit@5, hit@10, hit@20, hit@50]\n",
       "Index: []"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "eval_metrics = [\n",
    "    \"mrr\",\n",
    "    \"map\",\n",
    "    \"rprecision\",\n",
    "    \"recall@5\",\n",
    "    \"recall@10\",\n",
    "    \"recall@20\",\n",
    "    \"recall@50\",\n",
    "    \"recall@100\",\n",
    "    \"hit@1\",\n",
    "    \"hit@3\",\n",
    "    \"hit@5\",\n",
    "    \"hit@10\",\n",
    "    \"hit@20\",\n",
    "    \"hit@50\",\n",
    "]\n",
    "eval_csv = pd.DataFrame(columns=[\"idx\", \"query_id\", \"pred_rank\"] + eval_metrics)\n",
    "eval_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function based on StarkQA evaluation\n",
    "# https://github.com/snap-stanford/stark/blob/main/stark_qa/evaluator.py\n",
    "def evaluate(candidate_ids: List[int],\n",
    "             pred_ids: List[int],\n",
    "             pred: torch.Tensor,\n",
    "             answer_ids : Union[torch.LongTensor, List[int]],\n",
    "             metrics: List[str] = ['mrr', 'hit@3', 'recall@20'],\n",
    "             device: str = 'cpu') -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model predictions.\n",
    "\n",
    "    Args:\n",
    "        candidate_ids: List of candidate node ids.\n",
    "        pred_ids: List of predicted node ids.\n",
    "        pred: List of predicted node names.\n",
    "        answer_ids: List of correct node ids.\n",
    "        metrics: List of metrics to compute.\n",
    "        device: Device to use.\n",
    "    \"\"\"\n",
    "    all_pred = torch.ones((max(candidate_ids) + 1, pred.shape[1]), dtype=torch.float) * (pred.min() - 1)\n",
    "    all_pred[pred_ids, :] = pred\n",
    "    all_pred = all_pred[candidate_ids].t().to(device)\n",
    "\n",
    "    bool_gd = torch.zeros((max(candidate_ids) + 1, pred.shape[1]), dtype=torch.bool)\n",
    "    bool_gd[torch.concat(answer_ids), torch.repeat_interleave(torch.arange(len(answer_ids)), torch.tensor(list(map(len, answer_ids))))] = True\n",
    "    bool_gd = bool_gd[candidate_ids].t().to(device)\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(answer_ids)):\n",
    "        eval_metrics = {}\n",
    "        for metric in metrics:\n",
    "            k = int(metric.split('@')[-1]) if '@' in metric else None\n",
    "            if metric == 'mrr':\n",
    "                result = retrieval_reciprocal_rank(all_pred[i], bool_gd[i])\n",
    "            elif metric == 'rprecision':\n",
    "                result = retrieval_r_precision(all_pred[i], bool_gd[i])\n",
    "            elif 'hit' in metric:\n",
    "                result = retrieval_hit_rate(all_pred[i], bool_gd[i], top_k=k)\n",
    "            elif 'recall' in metric:\n",
    "                result = retrieval_recall(all_pred[i], bool_gd[i], top_k=k)\n",
    "            elif 'precision' in metric:\n",
    "                result = retrieval_precision(all_pred[i], bool_gd[i], top_k=k)\n",
    "            elif 'map' in metric:\n",
    "                result = retrieval_average_precision(all_pred[i], bool_gd[i], top_k=k)\n",
    "            elif 'ndcg' in metric:\n",
    "                result = retrieval_normalized_dcg(all_pred[i], bool_gd[i], top_k=k)\n",
    "            eval_metrics[metric] = float(result)\n",
    "        results.append(eval_metrics)\n",
    "    return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Similarity Search (VSS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particular model that we will evaluate is a simple vector similarity model, called vector similarity search (VSS).\n",
    "\n",
    "It measures the similarity between the embeddings of the query against the nodes of StarkQA-PrimeKG to retrieve the answer candidates.\n",
    "\n",
    "Please refer to the paper and the following code:\n",
    "- https://github.com/snap-stanford/stark/blob/main/stark_qa/models/vss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "split = 'test-0.1' # For simplicity, we use the small set of test data\n",
    "batch_size = 256\n",
    "model = 'vss'\n",
    "save_topk = 100 # Top-K predictions to be considered\n",
    "\n",
    "# Use testing split indices\n",
    "indices = starkqa_split_indices[split].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check device availability\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:30<00:00, 15.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prepare variables for evaluation\n",
    "candidate_ids = torch.LongTensor(primekg_nodes.node_id.tolist())\n",
    "\n",
    "# Loop through the test data\n",
    "for batch_idx in tqdm(range(0, len(indices), batch_size or len(indices))):\n",
    "    if batch_idx == 0:\n",
    "        batch_indices = [idx for idx in indices[batch_idx : min(batch_idx + batch_size, len(indices))]]\n",
    "        if len(batch_indices) == 0:\n",
    "            continue\n",
    "        # Get the query ids, queries, queries_embedded, and answer ids from dataframe\n",
    "        query_ids, queries, queries_embedded, answer_ids = zip(\n",
    "            *[starkqa_df[['id', 'query', 'query_embedded', 'answer_ids']].iloc[idx] for idx in batch_indices]\n",
    "        )\n",
    "        # Using VSS, we calculate similarities between query and candidate embeddings\n",
    "        similarity = torch.matmul(torch.tensor(np.array(queries_embedded)).to(device), \n",
    "                                  torch.tensor(np.array(primekg_nodes.x.values.tolist())).T.to(device)).cpu()\n",
    "        \n",
    "        # Measure performance\n",
    "        pred_ids = candidate_ids\n",
    "        pred = similarity.t()\n",
    "        answer_ids = [torch.LongTensor(answer_id) for answer_id in answer_ids]\n",
    "        results = evaluate(candidate_ids, pred_ids, pred, answer_ids, metrics=eval_metrics)\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            result[\"idx\"], result[\"query_id\"] = batch_indices[i], query_ids[i]\n",
    "            result[\"pred_rank\"] = pred_ids[torch.argsort(pred[:,i], descending=True)[save_topk]].tolist()\n",
    "            eval_csv = pd.concat([eval_csv, pd.DataFrame([result]).astype(eval_csv.dtypes)], ignore_index=True)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further check the evaluation results within the `eval_csv` dataframe to observe the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>query_id</th>\n",
       "      <th>pred_rank</th>\n",
       "      <th>mrr</th>\n",
       "      <th>map</th>\n",
       "      <th>rprecision</th>\n",
       "      <th>recall@5</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>recall@20</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>recall@100</th>\n",
       "      <th>hit@1</th>\n",
       "      <th>hit@3</th>\n",
       "      <th>hit@5</th>\n",
       "      <th>hit@10</th>\n",
       "      <th>hit@20</th>\n",
       "      <th>hit@50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3163</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.038068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>31190</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>99533</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>36650</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>231</td>\n",
       "      <td>22840</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>9977</td>\n",
       "      <td>9977</td>\n",
       "      <td>71495</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>9992</td>\n",
       "      <td>9992</td>\n",
       "      <td>72393</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>9996</td>\n",
       "      <td>9996</td>\n",
       "      <td>103422</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>10139</td>\n",
       "      <td>10139</td>\n",
       "      <td>127757</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>10191</td>\n",
       "      <td>10191</td>\n",
       "      <td>22104</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx query_id pred_rank       mrr       map rprecision recall@5  \\\n",
       "0        9        9      3163  0.090909  0.038068        0.0      0.0   \n",
       "1       26       26     31190  0.333333  0.333333        0.0      1.0   \n",
       "2       88       88     99533  0.076923  0.076923        0.0      0.0   \n",
       "3      195      195     36650  0.008696  0.008696        0.0      0.0   \n",
       "4      231      231     22840  0.010638  0.010638        0.0      0.0   \n",
       "..     ...      ...       ...       ...       ...        ...      ...   \n",
       "251   9977     9977     71495  0.000312  0.000312        0.0      0.0   \n",
       "252   9992     9992     72393  0.012658  0.006716        0.0      0.0   \n",
       "253   9996     9996    103422  0.000014  0.000014        0.0      0.0   \n",
       "254  10139    10139    127757  0.034483  0.034483        0.0      0.0   \n",
       "255  10191    10191     22104  0.000015  0.000015        0.0      0.0   \n",
       "\n",
       "    recall@10 recall@20 recall@50 recall@100 hit@1 hit@3 hit@5 hit@10 hit@20  \\\n",
       "0         0.0  0.333333  0.333333   0.333333   0.0   0.0   0.0    0.0    1.0   \n",
       "1         1.0       1.0       1.0        1.0   0.0   1.0   1.0    1.0    1.0   \n",
       "2         0.0       1.0       1.0        1.0   0.0   0.0   0.0    0.0    1.0   \n",
       "3         0.0       0.0       0.0        0.0   0.0   0.0   0.0    0.0    0.0   \n",
       "4         0.0       0.0       0.0        1.0   0.0   0.0   0.0    0.0    0.0   \n",
       "..        ...       ...       ...        ...   ...   ...   ...    ...    ...   \n",
       "251       0.0       0.0       0.0        0.0   0.0   0.0   0.0    0.0    0.0   \n",
       "252       0.0       0.0       0.0        0.5   0.0   0.0   0.0    0.0    0.0   \n",
       "253       0.0       0.0       0.0        0.0   0.0   0.0   0.0    0.0    0.0   \n",
       "254       0.0       0.0       1.0        1.0   0.0   0.0   0.0    0.0    0.0   \n",
       "255       0.0       0.0       0.0        0.0   0.0   0.0   0.0    0.0    0.0   \n",
       "\n",
       "    hit@50  \n",
       "0      1.0  \n",
       "1      1.0  \n",
       "2      1.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "..     ...  \n",
       "251    0.0  \n",
       "252    0.0  \n",
       "253    0.0  \n",
       "254    1.0  \n",
       "255    0.0  \n",
       "\n",
       "[256 rows x 17 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the evaluation results\n",
    "eval_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take the average of the evaluation metrics over the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mrr           0.127865\n",
       "map           0.085517\n",
       "rprecision    0.049698\n",
       "recall@5      0.113364\n",
       "recall@10     0.171124\n",
       "recall@20     0.240542\n",
       "recall@50     0.316177\n",
       "recall@100    0.365272\n",
       "hit@1         0.070312\n",
       "hit@3         0.140625\n",
       "hit@5         0.183594\n",
       "hit@10            0.25\n",
       "hit@20        0.324219\n",
       "hit@50        0.414062\n",
       "dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the mean of the evaluation metrics\n",
    "eval_csv[eval_metrics].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
